{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Gradient_Descent_Steepest_Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarshanPatel0919/Machine-Learning/blob/master/Gradient_Descent_Steepest_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJPH-N-fbhk_",
        "colab_type": "code",
        "outputId": "d14f903d-2666-43d5-99bf-858ba1820553",
        "colab": {}
      },
      "source": [
        "########################  Q.1 ########################\n",
        "\n",
        "import numpy as np\n",
        "import sympy as sp \n",
        "import pandas as pd\n",
        "import random as rd\n",
        "from scipy.misc import derivative as drv\n",
        "\n",
        "eps=1e-5\n",
        "dt=1e-9\n",
        "\n",
        "d=pd.read_csv('data.csv')\n",
        "\n",
        "x=np.array(d['x']) #to convert array into matrix put element-wise [] or use [np.newaxis] \n",
        "y=np.array(d['y']) #:= Useful when transpose operation involved on 1d array,here no need of that situaton\n",
        "\n",
        "x=(x - np.mean(x)) / np.std(x)           #normalizing the data\n",
        "x=np.array([[1,i] for i in x])           #appending column of 1's as prefix\n",
        "m=len(x)\n",
        "\n",
        "\n",
        "#Note:When passing a numpy array to function make sure to pass original objects copy if it is supposed to be pass by value.  \n",
        "\n",
        "def f(t):\n",
        "    #t-=1                                #keep for c and d\n",
        "    #t=t**2                              #keep for a,b,c,d\n",
        "    #t*=2                                #keep for d\n",
        "    t=np.sum((y-np.matmul(x,t))**2)/m   #keep for e\n",
        "    return np.sum(t) \n",
        "  \n",
        "def fd(t,i):\n",
        "    tn=np.array([t[p]+dt if i==p else t[p] for p in range(len(t))])\n",
        "    return (f(np.copy(tn))-f(np.copy(t)))/dt\n",
        "\n",
        "\n",
        "def gradient_rec(t0,n,a):                #maximum allowed recursion depth is approx 3000 or less\n",
        "    if n==0:return t0\n",
        "    t1 = t0 - a*np.array([fd(t0,i) for i in range(len(t0))])\n",
        "    return gradient_rec(t1,n-1,a)\n",
        "\n",
        "def gradient_itr(t0,n,a):\n",
        "    c=0\n",
        "    while n:\n",
        "        c+=1\n",
        "        t1 = t0 - (a*np.array([fd(t0,i) for i in range(len(t0))]))\n",
        "        if abs(np.sum(t1-t0))<eps:return (t1,c)\n",
        "        t0 = t1\n",
        "        n-=1\n",
        "    return (t0,c)\n",
        "def gradient_st(t0,n,a):                  #Stochastic\n",
        "    c=0\n",
        "    while n:\n",
        "        j = rd.randint(0,len(x)-1)\n",
        "        t1 = t0 - ((a/(len(x)))*np.array([(-(y[j]-np.matmul(x[j],t0))*x[j][i]) for i in range(len(t0))]))\n",
        "        c+=1\n",
        "        if abs(np.sum(t1-t0))<1e-9:return (t1,c)\n",
        "        t0 = t1\n",
        "        n-=1\n",
        "    return (t0,c)\n",
        "\n",
        "#t0=np.array([5])  #for single variable:a,c\n",
        "t0=np.array([5,5]) #for bi-variale:b,d\n",
        "a=[0.01,0.05,0.1]\n",
        "ni=[1000] #for single value of n\n",
        "ns=[100000]\n",
        "#n=[5,10,20,50,100,500,1000] #for more than on values of n\n",
        "\n",
        "for i in range(len(n)):\n",
        "    for j in range(len(a)):\n",
        "        t_best=gradient_itr(t0,ni[i],a[j])\n",
        "        print('Gradient descent : for n =',ni[i],'learning rate=',a[j],':',t_best)\n",
        "        t_best=gradient_st(t0,ns[i],a[j])\n",
        "        print('Stochastic Gradient descent : for n =',ns[i],'learning rate=',a[j],':',t_best)\n",
        "        #l.append(t_best)\n",
        "    #matrix.append(l)\n",
        "#print(matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient descent : for n = 1000 learning rate= 0.01 : (array([23.71676041, -6.8661557 ]), 473)\n",
            "Stochastic Gradient descent : for n = 100000 learning rate= 0.01 : (array([23.69460365, -6.78978018]), 100000)\n",
            "Gradient descent : for n = 1000 learning rate= 0.05 : (array([23.71787116, -6.86685985]), 108)\n",
            "Stochastic Gradient descent : for n = 100000 learning rate= 0.05 : (array([23.75428368, -6.81317784]), 100000)\n",
            "Gradient descent : for n = 1000 learning rate= 0.1 : (array([23.7179765 , -6.86692558]), 54)\n",
            "Stochastic Gradient descent : for n = 100000 learning rate= 0.1 : (array([23.73458412, -6.95519014]), 100000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86xxrKa3bhlI",
        "colab_type": "code",
        "outputId": "db6d39f8-e8db-4504-f8c0-9a76cd09e088",
        "colab": {}
      },
      "source": [
        "########################  Q.3 ########################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "from scipy.misc import derivative as drv\n",
        "\n",
        "eps=1e-5\n",
        "dt=1e-5        #choose dt such that it's any power does not exceed 11.Reason : computational limit\n",
        "\n",
        "d=pd.read_csv('data.csv')\n",
        "\n",
        "x=np.array(d['x']) #to convert array into matrix put element-wise [] or use [np.newaxis] \n",
        "y=np.array(d['y']) #:= Useful when transpose operation involved on 1d array,here no need of that situaton\n",
        "\n",
        "x=(x - np.mean(x)) / np.std(x) #normalizing the data\n",
        "\n",
        "x=np.array([[1,i] for i in x]) #appending column of 1's as prefix\n",
        "m=len(x)\n",
        "\n",
        "def f(t):\n",
        "    #t-=1                                #keep for c and d\n",
        "    #t=t**2                              #keep for a,b,c,d\n",
        "    #t*=2                                #keep for d\n",
        "    t=np.sum((y-np.matmul(x,t))**2)/m   #keep for e\n",
        "    return np.sum(t) \n",
        "\n",
        "def fd(t,i):\n",
        "    tn=np.array([t[p]+dt if i==p else t[p] for p in range(len(t))])\n",
        "    return (f(np.copy(tn))-f(np.copy(t)))/dt\n",
        "\n",
        "def jd(t,a,i):\n",
        "    if i==0:return f(t - a*np.array([fd(t,i) for i in range(len(t))]))\n",
        "    return (jd(t,a+dt,i-1)-jd(t,a,i-1))/dt\n",
        "\n",
        "def steepest(t0,a0):#maximum allowed recursion depth is approx 3000 or less\n",
        "    t1 = t0 - a0*np.array([fd(t0,i) for i in range(len(t0))])\n",
        "    if(jd(t0,a0,2)<eps):return (t0,a0)\n",
        "    a1 = a0 - jd(t0,a0,1)/jd(t0,a0,2)\n",
        "    if abs(a1-a0)<eps:return (t1,a1)\n",
        "    return steepest(t1,a1)\n",
        "\n",
        "#t0=np.array([5])  #for single variable:a,c\n",
        "t0=np.array([5,5]) #for bi-variale:b,d\n",
        "\n",
        "a=[0.01,0.05,0.1]\n",
        "#n=[5,10,20,50,100,500,1000] #for more than on values of n\n",
        "#matrix=[]\n",
        "#l=[]\n",
        "for j in range(len(a)):\n",
        "    t_best=steepest(t0,a[j])\n",
        "    print('for learning rate =',a[j],': Theta =',t_best[0],'and Alpha =',t_best[1])\n",
        "    #l.append(t_best)\n",
        "#matrix.append(l)\n",
        "#print(matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for learning rate = 0.01 : Theta = [23.71790826 -6.86689152] and Alpha = 0.4999950354985553\n",
            "for learning rate = 0.05 : Theta = [23.71791276 -6.86689434] and Alpha = 0.4999950386202284\n",
            "for learning rate = 0.1 : Theta = [23.71793481 -6.86690834] and Alpha = 0.49999504367986375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}